{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import json\n",
    "import pandas as pd\n",
    " \n",
    "path='/users/ashokpaliwal/Desktop/NLP_Project1/data'\n",
    " \n",
    "# Processing JSON files\n",
    "json_dir = path + '/docs'\n",
    "json_files = [f for f in listdir(json_dir) if isfile(join(json_dir, f))]\n",
    "input_data = []\n",
    "for i in range(len(json_files)):\n",
    "    file = json_dir + '/' + json_files[i]\n",
    "    with open(file) as f:  \n",
    "        data = json.load(f)\n",
    "        doc_info = [data[\"_id\"], data[\"jd_information\"][\"description\"]]\n",
    "        input_data.append(doc_info)\n",
    " \n",
    "json_data = pd.DataFrame(input_data)\n",
    "json_data.columns = ['Document ID', 'JD']\n",
    "json_data['Document ID'] = json_data['Document ID'].astype('int64')\n",
    "json_data['JD'] = json_data['JD'].astype(str)\n",
    " \n",
    "# Importing CSV file\n",
    "depts = pd.read_csv(path + '/' + 'document_departments.csv')\n",
    " \n",
    "# Joining CSV and JSON datasets to form training dataset\n",
    "full_data = pd.merge(json_data, depts, on = 'Document ID', how = 'left')\n",
    " \n",
    "# Testing empty JDs\n",
    "#j=1\n",
    "#for i in range(len(full_data['JD'])):\n",
    "#    JD = full_data['JD'][i]\n",
    "#    if JD == '': \n",
    "#        print(j, i)\n",
    "#        j = j + 1\n",
    " \n",
    "#TBD - Remove rows with emmpty JDs\n",
    "full_data = full_data[full_data['JD'] != '']\n",
    "#Randomizing dataset for test and train separation\n",
    "full_data = full_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "train_data = list(full_data['JD'].values)\n",
    "\n",
    "## load data\n",
    "#filename = 'metamorphosis_clean.txt'\n",
    "#file = open(filename, 'rt')\n",
    "#text = file.read()\n",
    "#file.close()\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# i want this code to go through every string value(jd)\n",
    "#[] for i in range(len(train_data))\n",
    "\n",
    "\n",
    "train_data2 = []\n",
    "\n",
    "\n",
    "for i in train_data:\n",
    "    ## split into words\n",
    "    tokens = word_tokenize(i)\n",
    "    ## convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    ## remove punctuation from each word\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    ## remove remaining tokens that are not alphabetic\n",
    "    word = [word for word in stripped if word.isalpha()]\n",
    "    ## filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in word if not w in stop_words]\n",
    "    #i = words\n",
    "    train_data2.append(words)\n",
    "\n",
    "\n",
    "t = keras.preprocessing.text.Tokenizer(num_words=10000)\n",
    "t.fit_on_texts(train_data2)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "encoded_data = t.texts_to_sequences(train_data2)\n",
    "max_doc_len = len(max(encoded_data, key=len)) + 1\n",
    "padded_data = keras.preprocessing.sequence.pad_sequences(\n",
    "        encoded_data, maxlen = max_doc_len, padding='post')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "labels = list(full_data['Department'].values)\n",
    "unique_labels = list(set(labels))\n",
    "label_indexes = dict()\n",
    "for i in range(len(unique_labels)):\n",
    "    label_indexes[i] = unique_labels[i]\n",
    " \n",
    "encoded_labels = []\n",
    "for label in labels:\n",
    "    for val, word in label_indexes.items():\n",
    "        if word == label:\n",
    "            encoded_labels.append(val)\n",
    " \n",
    "#Import GloVe mappings\n",
    "path2='/users/ashokpaliwal/Desktop/NLP_Project1/glove.6B'\n",
    "embeddings_index = dict()\n",
    "f = open(path2 + '/' + 'glove.6B.50d.txt', encoding = 'utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    " \n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size, 50))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    " \n",
    "#Build the model\n",
    "e = keras.layers.Embedding(vocab_size, 50, weights=[embedding_matrix], \n",
    "                           input_length=max_doc_len, trainable=False)\n",
    "model = keras.Sequential()\n",
    "model.add(e)\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(256, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(len(unique_labels), activation=tf.nn.softmax))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    " \n",
    "partial_train_data = padded_data[300:600]\n",
    "partial_train_labels = encoded_labels[300:600]\n",
    " \n",
    "partial_test_data = padded_data[:300]\n",
    "partial_test_labels = encoded_labels[:300]\n",
    " \n",
    "x_val = padded_data[600:]\n",
    "y_val = encoded_labels[600:]\n",
    " \n",
    "history = model.fit(partial_train_data, partial_train_labels, epochs=30, \n",
    "                    batch_size = 512, validation_data = (x_val, y_val), verbose=1)\n",
    "# evaluate the model\n",
    "results = model.evaluate(partial_test_data, partial_test_labels, verbose=0)\n",
    "print(results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
