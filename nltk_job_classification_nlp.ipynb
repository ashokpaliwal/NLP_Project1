{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing required packages for processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import json\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigating to input files directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd() + '/data'\n",
    "json_dir = path + '/docs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading and importing data containing job descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = [f for f in listdir(json_dir) if isfile(join(json_dir, f))]\n",
    "input_data = []\n",
    "for i in range(len(json_files)):\n",
    "    file = json_dir + '/' + json_files[i]\n",
    "    with open(file) as f:  \n",
    "        data = json.load(f)\n",
    "        doc_info = [data[\"_id\"], data[\"jd_information\"][\"description\"]]\n",
    "        input_data.append(doc_info)\n",
    "\n",
    "json_data = pd.DataFrame(input_data)\n",
    "json_data.columns = ['Document ID', 'JD']\n",
    "json_data['Document ID'] = json_data['Document ID'].astype('int64')\n",
    "json_data['JD'] = json_data['JD'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking that text strings are processed correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document ID</th>\n",
       "      <th>JD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8126421</td>\n",
       "      <td>Hiring 3D Designer for Exhibitions and Stall D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8260214</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8136071</td>\n",
       "      <td>&amp;nbsp;&amp;nbsp;Intrested candidates can drop thei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8337941</td>\n",
       "      <td>AL- HAMD CONSULTANT SERVICEBHAJANPURA NEW DELH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8370930</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document ID                                                 JD\n",
       "0      8126421  Hiring 3D Designer for Exhibitions and Stall D...\n",
       "1      8260214                                                   \n",
       "2      8136071  &nbsp;&nbsp;Intrested candidates can drop thei...\n",
       "3      8337941  AL- HAMD CONSULTANT SERVICEBHAJANPURA NEW DELH...\n",
       "4      8370930                                                   "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing file containing job classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "depts = pd.read_csv(path + '/' + 'document_departments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging job descriptions to their respective classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.merge(json_data, depts, on = 'Document ID', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viewing a snapshot of all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document ID</th>\n",
       "      <th>JD</th>\n",
       "      <th>Department</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8126421</td>\n",
       "      <td>Hiring 3D Designer for Exhibitions and Stall D...</td>\n",
       "      <td>Marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8260214</td>\n",
       "      <td></td>\n",
       "      <td>Sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8136071</td>\n",
       "      <td>&amp;nbsp;&amp;nbsp;Intrested candidates can drop thei...</td>\n",
       "      <td>Ticketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8337941</td>\n",
       "      <td>AL- HAMD CONSULTANT SERVICEBHAJANPURA NEW DELH...</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8370930</td>\n",
       "      <td></td>\n",
       "      <td>Analytics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document ID                                                 JD Department\n",
       "0      8126421  Hiring 3D Designer for Exhibitions and Stall D...  Marketing\n",
       "1      8260214                                                         Sales\n",
       "2      8136071  &nbsp;&nbsp;Intrested candidates can drop thei...  Ticketing\n",
       "3      8337941  AL- HAMD CONSULTANT SERVICEBHAJANPURA NEW DELH...         IT\n",
       "4      8370930                                                     Analytics"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning data: Removing data points without any classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = full_data[full_data['JD'] != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomizing dataset row orders for test and train separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = full_data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing tensorflow libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list(full_data['JD'].values)\n",
    "train_data2 = []\n",
    "\n",
    "for i in train_data:\n",
    "    ## split into words\n",
    "    tokens = word_tokenize(i)\n",
    "    ## convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    ## remove punctuation from each word\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    ## remove remaining tokens that are not alphabetic\n",
    "    word = [word for word in stripped if word.isalpha()]\n",
    "    ## filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in word if not w in stop_words]\n",
    "    #i = words\n",
    "    train_data2.append(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting job description strings: converting words to numeric indexes (to match the input format of neural network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = keras.preprocessing.text.Tokenizer(num_words=10000)\n",
    "t.fit_on_texts(train_data2)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "encoded_data = t.texts_to_sequences(train_data2)\n",
    "max_doc_len = len(max(encoded_data, key=len)) + 1\n",
    "padded_data = keras.preprocessing.sequence.pad_sequences(\n",
    "        encoded_data, maxlen = max_doc_len, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting job classification labels from strings to numeric indexes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(full_data['Department'].values)\n",
    "unique_labels = list(set(labels))\n",
    "label_indexes = dict()\n",
    "for i in range(len(unique_labels)):\n",
    "    label_indexes[i] = unique_labels[i]\n",
    "encoded_labels = []\n",
    "for label in labels:\n",
    "    for val, word in label_indexes.items():\n",
    "        if word == label:\n",
    "            encoded_labels.append(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import GloVe mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "glove_file_path = os.getcwd()\n",
    "embeddings_index = dict()\n",
    "f = open(glove_file_path + '/' + 'glove.6B.50d.txt', encoding = 'utf8')\n",
    "for line in f:\n",
    "\tvalues = line.split()\n",
    "\tword = values[0]\n",
    "\tcoefs = np.asarray(values[1:], dtype='float32')\n",
    "\tembeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a weight matrix for words in training docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 50))\n",
    "for word, i in t.word_index.items():\n",
    "\tembedding_vector = embeddings_index.get(word)\n",
    "\tif embedding_vector is not None:\n",
    "\t\tembedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a basic neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/ashokpaliwal/anaconda3/envs/tensorflow_env/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "e = keras.layers.Embedding(vocab_size, 50, weights=[embedding_matrix], \n",
    "                           input_length=max_doc_len, trainable=False)\n",
    "model = keras.Sequential()\n",
    "model.add(e)\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(512, activation=tf.nn.relu))\n",
    "model.add(keras.layers.Dense(len(unique_labels), activation=tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarizing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 557, 50)           309100    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 27850)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               14259712  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 27)                13851     \n",
      "=================================================================\n",
      "Total params: 14,582,663\n",
      "Trainable params: 14,273,563\n",
      "Non-trainable params: 309,100\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating training, validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_test_data = padded_data[:300]\n",
    "partial_test_labels = encoded_labels[:300]\n",
    "\n",
    "partial_train_data = padded_data[300:600]\n",
    "partial_train_labels = encoded_labels[300:600]\n",
    "\n",
    "x_val = padded_data[600:]\n",
    "y_val = encoded_labels[600:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the model to training data using validation data for observing if model over/under-fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 300 samples, validate on 145 samples\n",
      "Epoch 1/20\n",
      "300/300 [==============================] - 1s 3ms/sample - loss: 0.0975 - acc: 0.9867 - val_loss: 2.2335 - val_acc: 0.4759\n",
      "Epoch 2/20\n",
      "300/300 [==============================] - 1s 3ms/sample - loss: 0.0755 - acc: 0.9867 - val_loss: 2.2679 - val_acc: 0.4759\n",
      "Epoch 3/20\n",
      "300/300 [==============================] - 1s 3ms/sample - loss: 0.0631 - acc: 0.9867 - val_loss: 2.3081 - val_acc: 0.4759\n",
      "Epoch 4/20\n",
      "300/300 [==============================] - 1s 3ms/sample - loss: 0.0564 - acc: 0.9900 - val_loss: 2.3493 - val_acc: 0.4690\n",
      "Epoch 5/20\n",
      "300/300 [==============================] - 1s 3ms/sample - loss: 0.0509 - acc: 0.9900 - val_loss: 2.3881 - val_acc: 0.4759\n",
      "Epoch 6/20\n",
      "300/300 [==============================] - 1s 3ms/sample - loss: 0.0453 - acc: 0.9900 - val_loss: 2.4245 - val_acc: 0.4759\n",
      "Epoch 7/20\n",
      "300/300 [==============================] - 1s 3ms/sample - loss: 0.0396 - acc: 0.9900 - val_loss: 2.4621 - val_acc: 0.4897\n",
      "Epoch 8/20\n",
      "300/300 [==============================] - 1s 3ms/sample - loss: 0.0355 - acc: 0.9900 - val_loss: 2.5005 - val_acc: 0.4966\n",
      "Epoch 9/20\n",
      "300/300 [==============================] - 1s 3ms/sample - loss: 0.0344 - acc: 0.9933 - val_loss: 2.5343 - val_acc: 0.4966\n",
      "Epoch 10/20\n",
      "300/300 [==============================] - 1s 3ms/sample - loss: 0.0319 - acc: 0.9933 - val_loss: 2.5635 - val_acc: 0.4966\n",
      "Epoch 11/20\n",
      "300/300 [==============================] - 1s 3ms/sample - loss: 0.0286 - acc: 0.9933 - val_loss: 2.5918 - val_acc: 0.4966\n",
      "Epoch 12/20\n",
      "300/300 [==============================] - 1s 3ms/sample - loss: 0.0274 - acc: 0.9933 - val_loss: 2.6190 - val_acc: 0.4966\n",
      "Epoch 13/20\n",
      "300/300 [==============================] - 1s 3ms/sample - loss: 0.0266 - acc: 0.9933 - val_loss: 2.6457 - val_acc: 0.5103\n",
      "Epoch 14/20\n",
      "300/300 [==============================] - 1s 3ms/sample - loss: 0.0254 - acc: 0.9933 - val_loss: 2.6723 - val_acc: 0.5103\n",
      "Epoch 15/20\n",
      "300/300 [==============================] - 1s 3ms/sample - loss: 0.0236 - acc: 0.9933 - val_loss: 2.6990 - val_acc: 0.5172\n",
      "Epoch 16/20\n",
      "300/300 [==============================] - 1s 3ms/sample - loss: 0.0224 - acc: 0.9933 - val_loss: 2.7243 - val_acc: 0.5172\n",
      "Epoch 17/20\n",
      "300/300 [==============================] - 1s 3ms/sample - loss: 0.0224 - acc: 0.9933 - val_loss: 2.7450 - val_acc: 0.5172\n",
      "Epoch 18/20\n",
      "300/300 [==============================] - 1s 3ms/sample - loss: 0.0216 - acc: 0.9933 - val_loss: 2.7623 - val_acc: 0.5172\n",
      "Epoch 19/20\n",
      "300/300 [==============================] - 1s 3ms/sample - loss: 0.0204 - acc: 0.9933 - val_loss: 2.7790 - val_acc: 0.5172\n",
      "Epoch 20/20\n",
      "300/300 [==============================] - 1s 3ms/sample - loss: 0.0202 - acc: 0.9933 - val_loss: 2.7962 - val_acc: 0.5172\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_train_data, partial_train_labels, epochs=20, \n",
    "                    batch_size = 512, validation_data = (x_val, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evalutating the model accuracy over test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy achieved:  0.5366667\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(partial_test_data, partial_test_labels, verbose=0)\n",
    "print(\"Accuracy achieved: \",results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
